{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date: 2018.07.22\n",
    "# Author: Runyu Zhang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This demo demonstrate how to train a one layer, unsupervised NMF and one layer, supervised NMF, and how to analyze and visualize the \n",
    "result.\n",
    "\n",
    "In real implementation,  it might be helpful and of more flexibility to write your own training process \n",
    "instead of using the training functions in 'train.Ipynb'. This should not be too hard - you can simply\n",
    "make a few lines change in the functions in 'train.Ipynb' to create your own training function.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Ipynb_importer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ed4b2b0cefee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mIpynb_importer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mneural_nmf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNeural_NMF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEnergy_Loss_Func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL21_Norm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlsqnonneg_module\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLsqNonneg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'Ipynb_importer'"
     ]
    }
   ],
   "source": [
    "# loading packages and functions\n",
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import Ipynb_importer\n",
    "from neural_nmf import Neural_NMF, Energy_Loss_Func, L21_Norm\n",
    "from lsqnonneg_module import LsqNonneg\n",
    "from train import train_unsupervised, train_supervised\n",
    "#\n",
    "import torch.nn as nn\n",
    "from writer import Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading session\n",
    "from data_loading import X_BOW as X\n",
    "from data_loading import Label_BOW as label\n",
    "from data_loading import Y_BOW as Y\n",
    "from data_loading import L_BOW as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the network parameters\n",
    "m = X.shape[1]\n",
    "k1 = 20\n",
    "k2 = 15\n",
    "k3 = 10\n",
    "c = 9\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unsupervised case,one layer\n",
    "net = Deep_NMF([m, 9])\n",
    "loss_func = Energy_Loss_Func()\n",
    "# X_input = X*1000\n",
    "history_unsupervised = train_unsupervised(net, X_input, loss_func, epoch = 200, lr = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by calling history_unsupervised.get('variable_name'), you can get the variables that you recorded in the writer\n",
    "# getting these results might be helpful for debugging and choosing hyperparameters\n",
    "A1_lst = history_unsupervised.get('A1')\n",
    "S1_lst = history_unsupervised.get('S1')\n",
    "grad_A1_lst = history_unsupervised.get('grad_A1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the loss curve\n",
    "history_unsupervised.plot_scalar('loss')\n",
    "# plot the heatmap for S1\n",
    "history_unsupervised.plot_tensor('S1', [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The following code demonstrate the process of training a supervised one-layer NMF.\n",
    "\n",
    "The training for supervised NMF is a little bit tricky here, I am not sure if this is the best way to do this. Here I am using\n",
    "different learning rate for the NMF layer(lsqnonneg layer) and the classification layer. What's more, in the same epoch, the\n",
    "NMF layer and the classification layer are trained separately, the NMF will do one gradient update in one epoch, and the\n",
    "classification layer will do thirty. See more detail in the function 'train_supervised' in 'train.Ipynb'\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supervised case\n",
    "net = Deep_NMF([m, 9], 9)\n",
    "net.linear.weight.data = 1e-3*torch.randn(9,9,dtype = torch.double)\n",
    "loss_func = Energy_Loss_Func(lambd = 100000,classification_type = 'L2')\n",
    "X_input = X*1000\n",
    "history_supervised = train_supervised(net, X_input, loss_func, Y, epoch = 30, lr_nmf = 5000, lr_classification = 0.01, weight_decay = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the loss curve\n",
    "history_supervised.plot_scalar('loss')\n",
    "# plotting the heatmap of S1\n",
    "history_supervised.plot_tensor('S1', [-1])\n",
    "# getting the history for different varialbes\n",
    "A1_lst = history_supervised.get('A1')\n",
    "S1_lst = history_supervised.get('S1')\n",
    "grad_A1_lst = history_supervised.get('grad_A1')\n",
    "B_lst = history_supervised.get('weight')\n",
    "grad_B_lst = history_supervised.get('grad_weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_lst, pred = net(X)\n",
    "torch.argmax(pred, dim = 1) != label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(net.linear.weight.data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = X.shape[1]\n",
    "n = X.shape[0]\n",
    "k = 9\n",
    "net = Deep_NMF([m,9])\n",
    "loss_func = Energy_Loss_Func()\n",
    "X_input = 1000*X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = Deep_NMF([m, 12])\n",
    "loss_func = Energy_Loss_Func()\n",
    "X_input = X*1000\n",
    "epoch = 400\n",
    "lr = 1000\n",
    "for i in range(epoch):\n",
    "    net.zero_grad()\n",
    "    S_lst = net(X)\n",
    "    loss = loss_func(net, X_input, S_lst)\n",
    "    loss.backward()\n",
    "    history.add_scalar('loss', loss.data)\n",
    "    for l in range(net.depth - 1):\n",
    "        A = net.lsqnonneglst[l].A\n",
    "        # record history\n",
    "        history.add_tensor('A'+str(l+1), A.data)\n",
    "        history.add_tensor('grad_A'+str(l+1), A.grad.data)\n",
    "        history.add_tensor('S' + str(l+1), S_lst[l].data)\n",
    "        # projection gradient descent\n",
    "        A.data = A.data.sub_(lr*A.grad.data)\n",
    "        A.data = A.data.clamp(min = 0)\n",
    "    if (i+1)%10 == 0:\n",
    "        print('epoch = ', i+1, '\\n', loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (15,105))\n",
    "plt.imshow(S_lst[0].data.t())\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
