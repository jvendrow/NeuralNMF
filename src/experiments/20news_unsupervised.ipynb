{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, '../')\n",
    "\n",
    "from neural_nmf import Neural_NMF, Energy_Loss_Func, L21_Norm, Recon_Loss_Func\n",
    "from neural_nmf import LsqNonneg\n",
    "from neural_nmf import train_unsupervised, train_supervised\n",
    "#\n",
    "import torch.nn as nn\n",
    "from neural_nmf import Writer\n",
    "\n",
    "from time import time\n",
    "\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Preprocessing 20 Newsgroup Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove = ('headers','footers','quotes')\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list.extend(['thanks','edu','also','would','one','could','please','really','many','anyone','good','right','get','even','want','must','something','well','much','still','said','stay','away','first','looking','things','try','take','look','make','may','include','thing','like','two','or','etc','phone','oh','email'])\n",
    "\n",
    "categories = [\n",
    " 'comp.graphics',\n",
    " 'comp.sys.mac.hardware',\n",
    " 'misc.forsale',\n",
    " 'rec.motorcycles',\n",
    " 'rec.sport.baseball',\n",
    " 'sci.med',\n",
    " 'sci.space',\n",
    " 'talk.politics.guns',\n",
    " 'talk.politics.mideast',\n",
    " 'talk.religion.misc'\n",
    " ]\n",
    "\n",
    "\n",
    "#directory = \"categories_3\" #this was same with 50 per class\n",
    "#directory = \"categories_4\" #this is 20 per class\n",
    "directory = \"categories_5\" #this is 100 per class\n",
    "#directory = \"categories_6\" #this is 300 per class\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=remove)\n",
    "\n",
    "# remove numbers\n",
    "data_cleaned = [re.sub(r'\\d+','', file) for file in newsgroups_train.data]\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords_list)\n",
    "vectors = vectorizer.fit_transform(data_cleaned).transpose()\n",
    "idx_to_word = np.array(vectorizer.get_feature_names())\n",
    "\n",
    "X = vectors\n",
    "d, n = np.shape(X)\n",
    "\n",
    "Y = np.zeros((n))\n",
    "\n",
    "labels = {0:0, 1:0, 2:1, 3:2, 4:2, 5:3, 6:3, 7:4, 8:4, 9:5}\n",
    "\n",
    "for i in range(n-1):\n",
    "    label = newsgroups_train.target[i]\n",
    "    Y[i] = label\n",
    "\n",
    "X = torch.from_numpy(X.todense())\n",
    "Y = torch.from_numpy(Y).long()\n",
    "\n",
    "m = X.shape[0]\n",
    "k1 = 10\n",
    "k2 = 6\n",
    "\n",
    "\n",
    "sub = 100 #HOW MANY PER CLASS\n",
    "count = np.zeros((k1))\n",
    "\n",
    "X_new = torch.zeros((X.shape[0], sub*k1))\n",
    "Y_new = torch.zeros((sub*k1))\n",
    "j = 0\n",
    "for i in range(Y.shape[0]):\n",
    "    if(count[Y[i]] >= sub):\n",
    "        continue\n",
    "    count[Y[i]] += 1\n",
    "    X_new[:,j] = X[:,i]\n",
    "    Y_new[j] = labels[int(Y[i])]\n",
    "    j += 1\n",
    "\n",
    "X = X_new.double()\n",
    "Y = Y_new.long()\n",
    "\n",
    "ind = np.argsort(Y)\n",
    "X = X[:,ind]\n",
    "Y = Y[ind]\n",
    "\n",
    "split = 0.75\n",
    "L = torch.zeros((k2, Y.shape[0])).double()\n",
    "for i in range(len(categories)):\n",
    "    L[:,i*sub:i*sub+(int(split*sub))] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_mat = torch.zeros((6,Y.shape[0]))\n",
    "r = np.arange(Y.shape[0])\n",
    "Y_mat[[Y,r]] = 1\n",
    "Y_mat = Y_mat.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [28:15<00:00, 16.96s/it]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1695.5558469295502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [26:43<00:00, 16.03s/it]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1603.3017644882202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [25:27<00:00, 15.28s/it]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1527.9402220249176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [27:07<00:00, 16.28s/it]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1627.7277193069458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [25:29<00:00, 15.30s/it]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1529.9935314655304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [26:54<00:00, 16.15s/it]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1614.5999763011932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [26:43<00:00, 16.03s/it]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1603.0473515987396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [27:05<00:00, 16.25s/it]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1625.2378196716309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [26:56<00:00, 16.16s/it]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1616.0509881973267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [26:06<00:00, 15.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 1566.293996810913\n"
     ]
    }
   ],
   "source": [
    "lambd=1e-6\n",
    "optimizer=\"gd\"\n",
    "lr = 1e10\n",
    "\n",
    "epoch = 100\n",
    "class_iters=1\n",
    "weight_decay=0.995\n",
    "\n",
    "loss_func = Energy_Loss_Func(lambd=lambd) \n",
    "\n",
    "all_results = []\n",
    "\n",
    "for _ in range(10):\n",
    "    \n",
    "    start = time()\n",
    "    \n",
    "    net = Neural_NMF([m, k1, k2])\n",
    "\n",
    "    history_supervised = train_unsupervised(net, X, loss_func=loss_func, epoch = epoch, lr = lr, weight_decay=weight_decay, decay_epoch=5, optimizer=optimizer, full_history=True, verbose=True)\n",
    "    \n",
    "    end = time()\n",
    "\n",
    "    print(\"Training time: {}\".format(end-start))\n",
    "    \n",
    "    A1_lst = history_supervised.get('A1')\n",
    "    S1_lst = history_supervised.get('S1')\n",
    "    S2_lst = history_supervised.get('S2')\n",
    "\n",
    "    A2 = history_supervised.get('A2')[-1]\n",
    "\n",
    "    A1 = A1_lst[-1]\n",
    "    S1 = S1_lst[-1]\n",
    "    S2 = S2_lst[-1]\n",
    "\n",
    "\n",
    "    results = {}\n",
    "    results['X'] = X.detach().numpy()\n",
    "    results['Y'] = Y.detach().numpy()\n",
    "    results['A1'] = A1.detach().numpy()\n",
    "    results['S1'] = S1.detach().numpy()\n",
    "    results['A2'] = A2.detach().numpy()\n",
    "    results['S2'] = S2.detach().numpy()\n",
    "    results['words'] = idx_to_word\n",
    "    results['loss'] = np.asarray([float(x) for x in history_supervised.get('loss')])\n",
    "    \n",
    "    all_results.append(results)\n",
    "    \n",
    "np.save(\"unsupervsed_results_all\", all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_unknown_all_1 = 0\n",
    "acc_unknown_all_2 = 0\n",
    "\n",
    "for res in all_results:\n",
    "    A1 = res['A1']\n",
    "    A2 = res['A2']\n",
    "    S1 = res['S1']\n",
    "    S2 = res['S2']\n",
    "    \n",
    "    B = np.multiply(Y_mat.numpy(),L.numpy()) @ np.linalg.pinv(S1)\n",
    "    Y_pred = np.argmax(B @ S1, axis=0)\n",
    "    acc_unknown_1 = Y.numpy()[L[0]==0][Y_pred[L[0]==0]==Y.numpy()[L[0]==0]].shape[0]/ Y[L[0]==0].shape[0]\n",
    "    \n",
    "    acc_unknown_all_1 += acc_unknown_1\n",
    "    \n",
    "    B = np.multiply(Y_mat.numpy(),L.numpy()) @ np.linalg.pinv(S2)\n",
    "    Y_pred = np.argmax(B @ S2, axis=0)\n",
    "    acc_unknown_2 = Y.numpy()[L[0]==0][Y_pred[L[0]==0]==Y.numpy()[L[0]==0]].shape[0]/ Y[L[0]==0].shape[0]\n",
    "    \n",
    "    acc_unknown_all_2 += acc_unknown_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average layer 1 accuracy...    0.5983999999999999\n",
      "Average layer 2 accuracy...    0.5624\n"
     ]
    }
   ],
   "source": [
    "print(\"Average layer 1 accuracy...   \", acc_unknown_all_1/10)\n",
    "print(\"Average layer 2 accuracy...   \", acc_unknown_all_2/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2 = S2_lst[-1]\n",
    "S1 = S1_lst[-1]\n",
    "A2 = history_supervised.get('A2')[-1]\n",
    "\n",
    "A1 = A1_lst[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.multiply(Y_mat.numpy(),L.numpy()) @ torch.pinverse(S2).numpy()\n",
    "\n",
    "Y_pred = np.argmax(B @ S2.detach().numpy(), axis=0)\n",
    "\n",
    "\n",
    "print(\"Accuracy:          {}/{}\".format(Y.numpy()[Y_pred==Y.numpy()].shape[0], Y.shape[0]))\n",
    "print(\"Accuracy known:    {}/{}\".format(Y.numpy()[L[0]==1][Y_pred[L[0]==1]==Y.numpy()[L[0]==1]].shape[0], Y[L[0]==1].shape[0]))\n",
    "print(\"Accuracy unknown:  {}/{}\".format(Y.numpy()[L[0]==0][Y_pred[L[0]==0]==Y.numpy()[L[0]==0]].shape[0], Y[L[0]==0].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.multiply(Y_mat.numpy(),L.numpy()) @ torch.pinverse(S1).numpy()\n",
    "\n",
    "Y_pred = np.argmax(B @ S1.detach().numpy(), axis=0)\n",
    "\n",
    "\n",
    "print(\"Accuracy:          {}/{}\".format(Y.numpy()[Y_pred==Y.numpy()].shape[0], Y.shape[0]))\n",
    "print(\"Accuracy known:    {}/{}\".format(Y.numpy()[L[0]==1][Y_pred[L[0]==1]==Y.numpy()[L[0]==1]].shape[0], Y[L[0]==1].shape[0]))\n",
    "print(\"Accuracy unknown:  {}/{}\".format(Y.numpy()[L[0]==0][Y_pred[L[0]==0]==Y.numpy()[L[0]==0]].shape[0], Y[L[0]==0].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B1 = Y_mat.numpy() @ torch.pinverse(S1).numpy()\n",
    "Y_pred = np.argmax(B1 @ S1.detach().numpy(), axis=0)\n",
    "print(\"Accuracy: {}/{}\".format(Y.numpy()[Y_pred==Y.numpy()].shape[0], Y.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history_supervised.plot_scalar('loss_nmf')\n",
    "#history_supervised.plot_scalar('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_supervised.plot_scalar('loss_classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_supervised.plot_tensor('S1', [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_supervised.plot_tensor('S2',[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_supervised.plot_tensor('A2', [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate reconstruction error\n",
    "layer1_error = torch.norm(X - torch.mm(A1, S1))\n",
    "layer2_error = torch.norm(X - torch.mm(torch.mm(A1, A2), S2))\n",
    "\n",
    "print(\"Layer 1 error...    \" + str(layer1_error))\n",
    "print(\"Layer 2 error...    \" + str(layer2_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = np.empty((12,10), dtype=object)\n",
    "\n",
    "for i in range(keywords.shape[1]):\n",
    "    keywords[0,i] = \"Topic \" + str(i+1)\n",
    "    keywords[1,i] = \"-------\"\n",
    "\n",
    "for i in range(A1.shape[1]):\n",
    "    col = (A1[:,i]*7/6 -  torch.mean(A1, axis=1)).numpy()\n",
    "    top = col.argsort()\n",
    "    top = top[-10:][::-1]\n",
    "\n",
    "    keywords[2:,i] = idx_to_word[top] \n",
    "\n",
    "print(\"RANK 10 KEYWORDS:\")  \n",
    "print(\"------------------\")\n",
    "col_widths = [max([len(keywords[i][j]) for i in range(keywords.shape[0])])+2 for j in range(keywords.shape[1])]\n",
    "for row in keywords:\n",
    "    print(\"\".join(row[i].ljust(col_widths[i]) for i in range(0,5)))\n",
    "print(\"\")\n",
    "for row in keywords:\n",
    "    print(\"\".join(row[i].ljust(col_widths[i]) for i in range(5,10)))\n",
    "\n",
    "\n",
    "keywords = np.empty((12,6), dtype=object)\n",
    "\n",
    "for i in range(keywords.shape[1]):\n",
    "    keywords[0,i] = \"Topic \" + str(i+1)\n",
    "    keywords[1,i] = \"-------\"\n",
    "\n",
    "A = torch.mm(A1,A2)\n",
    "for i in range(A.shape[1]):\n",
    "    col = (A[:,i]*7/6 -  torch.mean(A, axis=1)).numpy()\n",
    "    top = col.argsort()\n",
    "    top = top[-10:][::-1]\n",
    "\n",
    "    keywords[2:,i] = idx_to_word[top] \n",
    "\n",
    "print(\"RANK 6 KEYWORDS:\")  \n",
    "print(\"------------------\")\n",
    "col_widths = [max([len(keywords[i][j]) for i in range(keywords.shape[0])])+2 for j in range(keywords.shape[1])]\n",
    "for row in keywords:\n",
    "    print(\"\".join(row[i].ljust(col_widths[i]) for i in range(len(row))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
